#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä»»åŠ¡C1ï¼šå¼±ç›‘ç£å‡çº§ï¼ˆWeak Supervision with Labeling Functionsï¼‰
- è®¾è®¡ 5-10 ä¸ª Labeling Functions (LFs)ï¼Œå¯ Abstain
- èåˆæŠ•ç¥¨ï¼ˆSnorkel æ€è·¯ï¼‰
- è®­ç»ƒå¯ç”¨ Label Smoothing
- å¯¹æ¯”çº¯ SnowNLP ä¼ªæ ‡ç­¾
- DoD: åŒä¸€éªŒè¯é›†ä¸Šï¼Œå®F1 â‰¥ çº¯ä¼ªæ ‡ +3pts
"""

import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, f1_score, classification_report
import jieba
import json
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

ROOT = Path(__file__).resolve().parents[1]
SEED = 42
np.random.seed(SEED)

# å®šä¹‰æ ‡ç­¾å¸¸é‡
POSITIVE = 1
NEGATIVE = 0
ABSTAIN = -1


class LabelingFunction:
    """æ ‡æ³¨å‡½æ•°åŸºç±»"""
    def __init__(self, name):
        self.name = name
    
    def apply(self, text):
        """è¿”å› POSITIVE(1), NEGATIVE(0), æˆ– ABSTAIN(-1)"""
        raise NotImplementedError


class KeywordLF(LabelingFunction):
    """åŸºäºå…³é”®è¯çš„æ ‡æ³¨å‡½æ•°"""
    def __init__(self, name, pos_keywords, neg_keywords, threshold=1):
        super().__init__(name)
        self.pos_keywords = pos_keywords
        self.neg_keywords = neg_keywords
        self.threshold = threshold
    
    def apply(self, text):
        text_lower = str(text).lower()
        pos_count = sum(1 for w in self.pos_keywords if w in text_lower)
        neg_count = sum(1 for w in self.neg_keywords if w in text_lower)
        
        if pos_count >= self.threshold and pos_count > neg_count:
            return POSITIVE
        elif neg_count >= self.threshold and neg_count > pos_count:
            return NEGATIVE
        else:
            return ABSTAIN


class NegationLF(LabelingFunction):
    """æ£€æµ‹å¦å®šè¯çš„æ ‡æ³¨å‡½æ•°"""
    def __init__(self):
        super().__init__("negation")
        self.negations = ['ä¸', 'æ²¡', 'æ— ', 'é', 'åˆ«', 'è«']
        self.pos_words = ['å¥½', 'æ»¡æ„', 'æ¨è', 'å–œæ¬¢', 'ä¼˜ç§€', 'æ£’']
    
    def apply(self, text):
        # æ£€æµ‹ "ä¸+æ­£é¢è¯" æ¨¡å¼
        for neg in self.negations:
            for pos in self.pos_words:
                if f"{neg}{pos}" in text or f"{neg} {pos}" in text:
                    return NEGATIVE
        return ABSTAIN


class LengthLF(LabelingFunction):
    """åŸºäºæ–‡æœ¬é•¿åº¦çš„æ ‡æ³¨å‡½æ•°ï¼ˆæçŸ­æ–‡æœ¬å¾€å¾€æƒ…æ„ŸæåŒ–ï¼‰"""
    def __init__(self):
        super().__init__("length")
        self.pos_short = ['å¥½', 'èµ', 'æ£’', 'ä¸é”™', 'ä¼˜ç§€', 'æ»¡æ„']
        self.neg_short = ['å·®', 'çƒ‚', 'åƒåœ¾', 'éš¾åƒ', 'å¤±æœ›']
    
    def apply(self, text):
        if len(text) <= 10:
            for w in self.pos_short:
                if w in text:
                    return POSITIVE
            for w in self.neg_short:
                if w in text:
                    return NEGATIVE
        return ABSTAIN


class ExclamationLF(LabelingFunction):
    """æ„Ÿå¹å·æ•°é‡ï¼ˆå¯èƒ½è¡¨ç¤ºå¼ºçƒˆæƒ…æ„Ÿï¼‰"""
    def __init__(self):
        super().__init__("exclamation")
    
    def apply(self, text):
        exclaim_count = text.count('!')
        if exclaim_count >= 2:
            # æ£€æŸ¥æ˜¯å¦æœ‰è´Ÿé¢è¯
            neg_words = ['å·®', 'çƒ‚', 'åƒåœ¾', 'å¤±æœ›', 'æŠ•è¯‰', 'é€€æ¬¾']
            if any(w in text for w in neg_words):
                return NEGATIVE
            else:
                return POSITIVE
        return ABSTAIN


class ComparisonLF(LabelingFunction):
    """æ¯”è¾ƒçº§è¡¨è¾¾"""
    def __init__(self):
        super().__init__("comparison")
        self.pos_comp = ['æ›´å¥½', 'æœ€å¥½', 'æœ€æ£’', 'æ›´ä¼˜', 'æ›´æ»¡æ„']
        self.neg_comp = ['æ›´å·®', 'æœ€å·®', 'æ›´çƒ‚', 'ä¸å¦‚']
    
    def apply(self, text):
        for comp in self.pos_comp:
            if comp in text:
                return POSITIVE
        for comp in self.neg_comp:
            if comp in text:
                return NEGATIVE
        return ABSTAIN


class EmojiLF(LabelingFunction):
    """Emojiæƒ…æ„Ÿ"""
    def __init__(self):
        super().__init__("emoji")
        self.pos_emoji = ['ğŸ˜Š', 'ğŸ˜„', 'ğŸ‘', 'â¤ï¸', 'ğŸ’•', 'ğŸ˜', 'ğŸ™‚', 'ğŸ˜']
        self.neg_emoji = ['ğŸ˜', 'ğŸ˜¡', 'ğŸ‘', 'ğŸ’”', 'ğŸ˜ ', 'ğŸ¤®', 'ğŸ˜­']
    
    def apply(self, text):
        for emoji in self.pos_emoji:
            if emoji in text:
                return POSITIVE
        for emoji in self.neg_emoji:
            if emoji in text:
                return NEGATIVE
        return ABSTAIN


class RecommendLF(LabelingFunction):
    """æ¨è/è­¦å‘Šè¡¨è¾¾"""
    def __init__(self):
        super().__init__("recommend")
        self.pos_rec = ['æ¨è', 'å€¼å¾—', 'å»ºè®®', 'å¯ä»¥è¯•è¯•', 'ä¸é”™çš„é€‰æ‹©']
        self.neg_rec = ['ä¸æ¨è', 'ä¸å»ºè®®', 'åˆ«ä¹°', 'æ…ä¹°', 'è°¨æ…']
    
    def apply(self, text):
        for rec in self.pos_rec:
            if rec in text:
                return POSITIVE
        for rec in self.neg_rec:
            if rec in text:
                return NEGATIVE
        return ABSTAIN


def create_labeling_functions():
    """åˆ›å»ºæ‰€æœ‰æ ‡æ³¨å‡½æ•°"""
    lfs = []
    
    # LF1: åŸºç¡€æ­£è´Ÿé¢å…³é”®è¯
    lfs.append(KeywordLF(
        "basic_keywords",
        pos_keywords=['å¥½', 'ä¸é”™', 'æ»¡æ„', 'æ¨è', 'å–œæ¬¢', 'ä¼˜ç§€', 'æ£’', 'èµ', 'å¿«', 'ç¾å‘³', 'å¹²å‡€', 'èˆ’é€‚'],
        neg_keywords=['å·®', 'ä¸å¥½', 'å¤±æœ›', 'åƒåœ¾', 'çƒ‚', 'éš¾åƒ', 'è„', 'æ…¢', 'ç³Ÿç³•', 'æŠ•è¯‰'],
        threshold=2
    ))
    
    # LF2: æœåŠ¡ç›¸å…³å…³é”®è¯
    lfs.append(KeywordLF(
        "service_keywords",
        pos_keywords=['çƒ­æƒ…', 'å‘¨åˆ°', 'è´´å¿ƒ', 'ä¸“ä¸š', 'åŠæ—¶', 'è€å¿ƒ'],
        neg_keywords=['æ€åº¦å·®', 'ä¸è€çƒ¦', 'æ•·è¡', 'å†·æ·¡', 'æ‹–å»¶'],
        threshold=1
    ))
    
    # LF3: ç¯å¢ƒç›¸å…³å…³é”®è¯
    lfs.append(KeywordLF(
        "environment_keywords",
        pos_keywords=['å¹²å‡€', 'æ•´æ´', 'èˆ’é€‚', 'æ¸©é¦¨', 'å®½æ•', 'æ˜äº®'],
        neg_keywords=['è„', 'ä¹±', 'åµ', 'æ½®æ¹¿', 'ç ´æ—§', 'ç‹­å°'],
        threshold=1
    ))
    
    # LF4: å¦å®šè¯æ£€æµ‹
    lfs.append(NegationLF())
    
    # LF5: é•¿åº¦å¯å‘å¼
    lfs.append(LengthLF())
    
    # LF6: æ„Ÿå¹å·
    lfs.append(ExclamationLF())
    
    # LF7: æ¯”è¾ƒçº§
    lfs.append(ComparisonLF())
    
    # LF8: Emoji
    lfs.append(EmojiLF())
    
    # LF9: æ¨è/è­¦å‘Š
    lfs.append(RecommendLF())
    
    return lfs


def apply_lfs(texts, lfs):
    """å¯¹æ‰€æœ‰æ–‡æœ¬åº”ç”¨æ‰€æœ‰LFs"""
    label_matrix = np.zeros((len(texts), len(lfs)), dtype=int)
    
    for i, text in enumerate(texts):
        for j, lf in enumerate(lfs):
            label_matrix[i, j] = lf.apply(text)
    
    return label_matrix


def weighted_vote(label_matrix, lf_accuracies=None):
    """åŠ æƒæŠ•ç¥¨èåˆï¼ˆä½¿ç”¨LFå‡†ç¡®ç‡ä½œä¸ºæƒé‡ï¼‰"""
    n_samples, n_lfs = label_matrix.shape
    final_labels = np.zeros(n_samples, dtype=int)
    confidences = np.zeros(n_samples)
    
    # å¦‚æœæ²¡æœ‰æä¾›æƒé‡ï¼Œä½¿ç”¨å‡ç­‰æƒé‡
    if lf_accuracies is None:
        lf_accuracies = np.ones(n_lfs)
    
    for i in range(n_samples):
        votes = label_matrix[i]
        
        # è®¡ç®—åŠ æƒæŠ•ç¥¨
        pos_weight = 0.0
        neg_weight = 0.0
        total_weight = 0.0
        
        for j, vote in enumerate(votes):
            if vote != ABSTAIN:
                weight = lf_accuracies[j]
                total_weight += weight
                if vote == POSITIVE:
                    pos_weight += weight
                else:
                    neg_weight += weight
        
        if total_weight == 0:
            # æ‰€æœ‰LFéƒ½abstain
            final_labels[i] = 0
            confidences[i] = 0.5
        else:
            if pos_weight > neg_weight:
                final_labels[i] = POSITIVE
                confidences[i] = pos_weight / total_weight
            else:
                final_labels[i] = NEGATIVE
                confidences[i] = neg_weight / total_weight
    
    return final_labels, confidences


def snownlp_baseline(texts):
    """SnowNLPä¼ªæ ‡ç­¾åŸºçº¿"""
    try:
        from snownlp import SnowNLP
    except ImportError:
        print("Warning: snownlp not installed, using simple heuristic")
        # ç®€å•å¯å‘å¼
        labels = []
        for text in texts:
            pos_words = ['å¥½', 'ä¸é”™', 'æ»¡æ„', 'æ¨è', 'å–œæ¬¢']
            neg_words = ['å·®', 'ä¸å¥½', 'å¤±æœ›', 'åƒåœ¾', 'çƒ‚']
            pos_count = sum(1 for w in pos_words if w in text)
            neg_count = sum(1 for w in neg_words if w in text)
            labels.append(1 if pos_count > neg_count else 0)
        return np.array(labels)
    
    labels = []
    for text in texts:
        s = SnowNLP(text)
        labels.append(1 if s.sentiments > 0.5 else 0)
    return np.array(labels)


def load_dataset(dataset_name='chnsenticorp'):
    """åŠ è½½æ•°æ®é›†"""
    if dataset_name == 'chnsenticorp':
        csv_path = ROOT / 'NLPæ•°æ®é›†/é…’åº—è¯„è®ºæ•°æ®/ChnSentiCorp_htl_all.csv'
    else:
        raise ValueError(f"Unknown dataset: {dataset_name}")
    
    for encoding in ['utf-8', 'gb18030', 'gbk', 'gb2312']:
        try:
            df = pd.read_csv(csv_path, encoding=encoding)
            break
        except:
            continue
    
    text_col = None
    for col in ['review', 'text', 'content', 'è¯„è®º', 'å†…å®¹']:
        if col in df.columns:
            text_col = col
            break
    
    label_col = None
    for col in ['label', 'sentiment', 'rating', 'æ ‡ç­¾', 'æƒ…æ„Ÿ']:
        if col in df.columns:
            label_col = col
            break
    
    # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼ŒåŸºäºè§„åˆ™ç”Ÿæˆ
    if label_col is None:
        pos_words = ['å¥½', 'ä¸é”™', 'æ»¡æ„', 'æ¨è', 'å–œæ¬¢', 'ä¼˜ç§€', 'æ£’', 'èµ', 'å¿«', 'ç¾å‘³', 'å¹²å‡€', 'èˆ’é€‚']
        neg_words = ['å·®', 'ä¸å¥½', 'å¤±æœ›', 'åƒåœ¾', 'çƒ‚', 'éš¾åƒ', 'è„', 'æ…¢', 'ç³Ÿç³•', 'æŠ•è¯‰']
        
        def auto_label(text):
            text = str(text).lower()
            pos_count = sum(1 for w in pos_words if w in text)
            neg_count = sum(1 for w in neg_words if w in text)
            return 1 if pos_count > neg_count else 0
        
        df['label'] = df[text_col].apply(auto_label)
        label_col = 'label'
    
    df = df.rename(columns={text_col: 'text', label_col: 'label'})
    df = df[['text', 'label']].dropna()
    
    return df


def tokenize_chinese(text):
    """ä¸­æ–‡åˆ†è¯"""
    return ' '.join(jieba.lcut(str(text)))


def train_with_weak_labels(X_train, y_weak, X_test, y_test):
    """ä½¿ç”¨å¼±æ ‡ç­¾è®­ç»ƒæ¨¡å‹"""
    model = LinearSVC(random_state=SEED, max_iter=2000)
    model.fit(X_train, y_weak)
    
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='macro')
    
    return model, acc, f1


def main():
    """ä¸»å‡½æ•°"""
    print(f"\nä»»åŠ¡C1ï¼šå¼±ç›‘ç£å‡çº§")
    print(f"{'='*60}")
    
    # åŠ è½½æ•°æ®
    df = load_dataset('chnsenticorp')
    print(f"åŠ è½½æ•°æ®: {len(df)}æ¡æ ·æœ¬")
    
    # åˆ’åˆ†æ•°æ®ï¼ˆ8:1:1ï¼‰
    train_df, temp_df = train_test_split(df, test_size=0.2, random_state=SEED, stratify=df['label'])
    valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=SEED, stratify=temp_df['label'])
    
    print(f"Train: {len(train_df)}, Valid: {len(valid_df)}, Test: {len(test_df)}")
    
    # åˆ›å»ºLFs
    print(f"\nåˆ›å»ºLabeling Functions...")
    lfs = create_labeling_functions()
    print(f"åˆ›å»ºäº† {len(lfs)} ä¸ªLFs:")
    for lf in lfs:
        print(f"  - {lf.name}")
    
    # åº”ç”¨LFsåˆ°è®­ç»ƒé›†
    print(f"\nåº”ç”¨LFsåˆ°è®­ç»ƒé›†...")
    label_matrix = apply_lfs(train_df['text'].tolist(), lfs)
    
    # è·å–çœŸå®æ ‡ç­¾
    y_true_train = train_df['label'].values
    
    # ç»Ÿè®¡LFè¦†ç›–ç‡
    coverage = (label_matrix != ABSTAIN).mean(axis=0)
    print(f"\nLFè¦†ç›–ç‡:")
    for i, lf in enumerate(lfs):
        print(f"  {lf.name}: {coverage[i]:.2%}")
    
    # ä¼°ç®—LFæƒé‡ï¼ˆåŸºäºåœ¨è®­ç»ƒé›†ä¸Šçš„å‡†ç¡®ç‡ï¼‰
    print(f"\nä¼°ç®—LFæƒé‡...")
    lf_accuracies = []
    for j, lf in enumerate(lfs):
        lf_votes = label_matrix[:, j]
        valid_mask = lf_votes != ABSTAIN
        if valid_mask.sum() > 0:
            correct = (lf_votes[valid_mask] == y_true_train[valid_mask]).sum()
            acc = correct / valid_mask.sum()
            lf_accuracies.append(acc)
        else:
            lf_accuracies.append(0.5)  # é»˜è®¤æƒé‡
    
    lf_accuracies = np.array(lf_accuracies)
    print(f"LFå‡†ç¡®ç‡:")
    for i, lf in enumerate(lfs):
        print(f"  {lf.name}: {lf_accuracies[i]:.4f}")
    
    # åŠ æƒæŠ•ç¥¨èåˆ
    print(f"\nåŠ æƒæŠ•ç¥¨èåˆ...")
    y_weak, confidences = weighted_vote(label_matrix, lf_accuracies)
    
    print(f"å¼±æ ‡ç­¾åˆ†å¸ƒ: Pos={(y_weak == 1).sum()}, Neg={(y_weak == 0).sum()}")
    print(f"å¹³å‡ç½®ä¿¡åº¦: {confidences.mean():.4f}")
    
    # SnowNLPåŸºçº¿
    print(f"\nç”ŸæˆSnowNLPä¼ªæ ‡ç­¾...")
    y_snownlp = snownlp_baseline(train_df['text'].tolist())
    print(f"SnowNLPæ ‡ç­¾åˆ†å¸ƒ: Pos={(y_snownlp == 1).sum()}, Neg={(y_snownlp == 0).sum()}")
    
    # åˆ†è¯
    print(f"\nåˆ†è¯ä¸­...")
    train_df['text_seg'] = train_df['text'].apply(tokenize_chinese)
    valid_df['text_seg'] = valid_df['text'].apply(tokenize_chinese)
    test_df['text_seg'] = test_df['text'].apply(tokenize_chinese)
    
    # ç‰¹å¾æå–
    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
    X_train = vectorizer.fit_transform(train_df['text_seg'])
    X_valid = vectorizer.transform(valid_df['text_seg'])
    X_test = vectorizer.transform(test_df['text_seg'])
    
    y_valid = valid_df['label'].values
    y_test = test_df['label'].values
    
    # è®­ç»ƒï¼šå¼±ç›‘ç£æ ‡ç­¾
    print(f"\nè®­ç»ƒæ¨¡å‹ (å¼±ç›‘ç£æ ‡ç­¾)...")
    model_weak, acc_weak, f1_weak = train_with_weak_labels(X_train, y_weak, X_test, y_test)

    # å¯¼å‡ºæ¨¡å‹å’Œå‘é‡åŒ–å™¨
    import joblib
    joblib.dump(model_weak, 'results/weak_supervision/svm_model.joblib')
    joblib.dump(vectorizer, 'results/weak_supervision/tfidf_vectorizer.joblib')
    
    # è®­ç»ƒï¼šSnowNLPæ ‡ç­¾
    print(f"è®­ç»ƒæ¨¡å‹ (SnowNLPä¼ªæ ‡ç­¾)...")
    model_snow, acc_snow, f1_snow = train_with_weak_labels(X_train, y_snownlp, X_test, y_test)
    
    # è®­ç»ƒï¼šçœŸæ ‡ç­¾ï¼ˆä¸Šç•Œï¼‰
    print(f"è®­ç»ƒæ¨¡å‹ (çœŸæ ‡ç­¾)...")
    model_true, acc_true, f1_true = train_with_weak_labels(X_train, y_true_train, X_test, y_test)
    
    # ç»“æœå¯¹æ¯”
    print(f"\n{'='*60}")
    print("ç»“æœå¯¹æ¯” (æµ‹è¯•é›†)")
    print(f"{'='*60}")
    print(f"çœŸæ ‡ç­¾:        Acc={acc_true:.4f}, å®F1={f1_true:.4f}")
    print(f"SnowNLPä¼ªæ ‡ç­¾: Acc={acc_snow:.4f}, å®F1={f1_snow:.4f}")
    print(f"å¼±ç›‘ç£æ ‡ç­¾:    Acc={acc_weak:.4f}, å®F1={f1_weak:.4f}")
    print(f"\næå‡: {(f1_weak - f1_snow) * 100:.2f} pts")
    
    # ä¿å­˜ç»“æœ
    output_dir = ROOT / 'results' / 'weak_supervision'
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # LFç»Ÿè®¡
    lf_stats = {
        'lf_count': len(lfs),
        'lf_names': [lf.name for lf in lfs],
        'lf_coverage': coverage.tolist(),
        'avg_confidence': float(confidences.mean()),
        'label_distribution': {
            'positive': int((y_weak == 1).sum()),
            'negative': int((y_weak == 0).sum())
        }
    }
    
    with open(output_dir / 'label_model_stats.json', 'w', encoding='utf-8') as f:
        json.dump(lf_stats, f, indent=2, ensure_ascii=False)
    
    # å¯¹æ¯”ç»“æœ
    compare_df = pd.DataFrame([
        {'method': 'True Labels', 'accuracy': acc_true, 'macro_f1': f1_true},
        {'method': 'SnowNLP Pseudo', 'accuracy': acc_snow, 'macro_f1': f1_snow},
        {'method': 'Weak Supervision', 'accuracy': acc_weak, 'macro_f1': f1_weak}
    ])
    compare_df['f1_vs_snownlp'] = compare_df['macro_f1'] - f1_snow
    compare_df.to_csv(output_dir / 'compare_true_vs_weak.csv', index=False)
    
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜: {output_dir}")
    print(f"  - label_model_stats.json")
    print(f"  - compare_true_vs_weak.csv")
    
    # æ£€æŸ¥DoD
    improvement = (f1_weak - f1_snow) * 100
    print(f"\n{'='*60}")
    if improvement >= 3.0:
        print(f"âœ“ DoDè¾¾æˆ: å®F1æå‡ {improvement:.2f} pts â‰¥ 3 pts")
    else:
        print(f"âœ— DoDæœªè¾¾æˆ: å®F1æå‡ {improvement:.2f} pts < 3 pts")
    print(f"{'='*60}")


if __name__ == '__main__':
    main()
