"""
Task D3: ABSAä¸‰å…ƒç»„æå–
ä½¿ç”¨DeepSeekå®ç°æ–¹é¢çº§æƒ…æ„Ÿåˆ†æï¼ˆAspect-Based Sentiment Analysisï¼‰
æå–ï¼ˆæ–¹é¢è¯ï¼Œè§‚ç‚¹è¯ï¼Œæƒ…æ„Ÿææ€§ï¼‰ä¸‰å…ƒç»„
"""

import os
import pandas as pd
import numpy as np
from pathlib import Path
from openai import OpenAI
import json
from datetime import datetime
from tqdm import tqdm
import time

class ABSAExtractor:
    """ABSAä¸‰å…ƒç»„æå–å™¨"""
    
    def __init__(self, api_key=None):
        """åˆå§‹åŒ–"""
        self.api_key = api_key or os.getenv('DEEPSEEK_API_KEY')
        if not self.api_key:
            raise ValueError("è¯·è®¾ç½®DEEPSEEK_API_KEYç¯å¢ƒå˜é‡")
        
        self.client = OpenAI(
            api_key=self.api_key,
            base_url="https://api.deepseek.com"
        )
    
    def extract_triplets(self, text, domain='hotel'):
        """
        æå–ABSAä¸‰å…ƒç»„
        
        Args:
            text: è¯„è®ºæ–‡æœ¬
            domain: é¢†åŸŸï¼ˆhotel/food/ecommerceï¼‰
        
        Returns:
            triplets: [{"aspect": "æœåŠ¡", "opinion": "æ€åº¦å¥½", "sentiment": "æ­£é¢"}, ...]
        """
        # æ ¹æ®é¢†åŸŸå®šåˆ¶æç¤ºè¯
        domain_aspects = {
            'hotel': 'æœåŠ¡ã€ç¯å¢ƒã€ä½ç½®ã€è®¾æ–½ã€ä»·æ ¼ã€å«ç”Ÿ',
            'food': 'å£å‘³ã€é…é€ã€åŒ…è£…ã€ä»·æ ¼ã€åˆ†é‡ã€å«ç”Ÿ',
            'ecommerce': 'è´¨é‡ã€ä»·æ ¼ã€ç‰©æµã€åŒ…è£…ã€å¤–è§‚ã€åŠŸèƒ½'
        }
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–¹é¢çº§æƒ…æ„Ÿåˆ†æï¼ˆABSAï¼‰ä¸“å®¶ã€‚è¯·ä»è¯„è®ºä¸­æå–ï¼ˆæ–¹é¢è¯ï¼Œè§‚ç‚¹è¯ï¼Œæƒ…æ„Ÿææ€§ï¼‰ä¸‰å…ƒç»„ã€‚

**ä»»åŠ¡è¯´æ˜ï¼š**
- æ–¹é¢è¯ï¼ˆAspectï¼‰ï¼šè¯„è®ºå…³æ³¨çš„å…·ä½“æ–¹é¢ï¼Œå¦‚{domain_aspects.get(domain, 'æœåŠ¡ã€è´¨é‡ã€ä»·æ ¼ç­‰')}
- è§‚ç‚¹è¯ï¼ˆOpinionï¼‰ï¼šæè¿°æ–¹é¢çš„å…·ä½“è¡¨è¾¾ï¼Œå¦‚"å¾ˆå¥½"ã€"å¤ªå·®"ã€"ä¸€èˆ¬"ç­‰
- æƒ…æ„Ÿææ€§ï¼ˆSentimentï¼‰ï¼šæ­£é¢/è´Ÿé¢/ä¸­æ€§

**è¾“å‡ºæ ¼å¼ï¼š**
ä¸¥æ ¼æŒ‰ç…§JSONæ•°ç»„æ ¼å¼è¾“å‡ºï¼Œä¾‹å¦‚ï¼š
[
  {{"aspect": "æœåŠ¡", "opinion": "æ€åº¦å¾ˆå¥½", "sentiment": "æ­£é¢"}},
  {{"aspect": "ä»·æ ¼", "opinion": "åè´µ", "sentiment": "è´Ÿé¢"}}
]

**æ³¨æ„äº‹é¡¹ï¼š**
1. å¦‚æœæ²¡æœ‰æ˜ç¡®çš„æ–¹é¢è¯ï¼Œä½¿ç”¨"æ•´ä½“"ä½œä¸ºæ–¹é¢è¯
2. å¦‚æœåŒä¸€æ–¹é¢æœ‰å¤šä¸ªè§‚ç‚¹ï¼Œåˆ†åˆ«æå–
3. åªè¾“å‡ºJSONï¼Œä¸è¦æœ‰ä»»ä½•é¢å¤–è§£é‡Š
"""
        
        user_prompt = f"è¯·åˆ†æä»¥ä¸‹è¯„è®ºçš„æ–¹é¢çº§æƒ…æ„Ÿï¼š\n\n{text}"
        
        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.2,
                max_tokens=600
            )
            
            result = response.choices[0].message.content.strip()
            
            # æ¸…ç†markdownä»£ç å—æ ‡è®°
            if result.startswith('```'):
                result = result.split('```')[1]
                if result.startswith('json'):
                    result = result[4:]
                result = result.strip()
            
            # è§£æJSON
            try:
                triplets = json.loads(result)
                
                # éªŒè¯æ ¼å¼
                if not isinstance(triplets, list):
                    return [], result
                
                # æ ‡å‡†åŒ–æ ¼å¼
                normalized = []
                for t in triplets:
                    if isinstance(t, dict) and 'aspect' in t and 'sentiment' in t:
                        normalized.append({
                            'aspect': t.get('aspect', ''),
                            'opinion': t.get('opinion', ''),
                            'sentiment': t.get('sentiment', '')
                        })
                
                return normalized, result
            
            except json.JSONDecodeError as e:
                print(f"  âš ï¸  JSONè§£æå¤±è´¥: {str(e)}")
                print(f"     åŸå§‹è¾“å‡º: {result[:200]}")
                return [], result
        
        except Exception as e:
            print(f"  âš ï¸  æå–å¤±è´¥: {str(e)}")
            return [], str(e)
    
    def extract_batch(self, texts, domain='hotel', max_samples=None):
        """æ‰¹é‡æå–"""
        print(f"\nğŸ”® æ‰¹é‡ABSAä¸‰å…ƒç»„æå–...")
        print(f"   é¢†åŸŸ: {domain}")
        print(f"   æ ·æœ¬æ•°: {len(texts) if max_samples is None else min(len(texts), max_samples)}")
        
        if max_samples is not None:
            texts = texts[:max_samples]
        
        results = []
        
        for text in tqdm(texts, desc="   æå–è¿›åº¦"):
            triplets, raw_output = self.extract_triplets(text, domain)
            
            results.append({
                'text': text,
                'triplets': triplets,
                'n_triplets': len(triplets),
                'raw_output': raw_output
            })
            
            # é™é€Ÿ
            time.sleep(0.8)
        
        return pd.DataFrame(results)

def analyze_absa_distribution(results_df):
    """åˆ†æABSAä¸‰å…ƒç»„çš„åˆ†å¸ƒ"""
    print(f"\nğŸ“Š ABSAåˆ†å¸ƒç»Ÿè®¡...")
    
    # å±•å¼€æ‰€æœ‰ä¸‰å…ƒç»„
    all_triplets = []
    for _, row in results_df.iterrows():
        if isinstance(row['triplets'], list):
            for t in row['triplets']:
                all_triplets.append(t)
    
    if len(all_triplets) == 0:
        print("   âš ï¸  æ²¡æœ‰æå–åˆ°æœ‰æ•ˆä¸‰å…ƒç»„")
        return None
    
    triplets_df = pd.DataFrame(all_triplets)
    
    # æ–¹é¢è¯åˆ†å¸ƒ
    print(f"\n   æ–¹é¢è¯åˆ†å¸ƒï¼ˆTop 10ï¼‰:")
    aspect_counts = triplets_df['aspect'].value_counts().head(10)
    for aspect, count in aspect_counts.items():
        print(f"     - {aspect}: {count}")
    
    # æƒ…æ„Ÿææ€§åˆ†å¸ƒ
    print(f"\n   æƒ…æ„Ÿææ€§åˆ†å¸ƒ:")
    sentiment_counts = triplets_df['sentiment'].value_counts()
    for sentiment, count in sentiment_counts.items():
        print(f"     - {sentiment}: {count} ({count/len(triplets_df)*100:.1f}%)")
    
    # æ–¹é¢-æƒ…æ„Ÿäº¤å‰è¡¨
    print(f"\n   æ–¹é¢-æƒ…æ„Ÿäº¤å‰è¡¨ï¼ˆTop 5æ–¹é¢ï¼‰:")
    top_aspects = triplets_df['aspect'].value_counts().head(5).index
    crosstab = pd.crosstab(
        triplets_df[triplets_df['aspect'].isin(top_aspects)]['aspect'],
        triplets_df[triplets_df['aspect'].isin(top_aspects)]['sentiment']
    )
    print(crosstab.to_string())
    
    return {
        'total_triplets': len(all_triplets),
        'unique_aspects': triplets_df['aspect'].nunique(),
        'aspect_distribution': aspect_counts.to_dict(),
        'sentiment_distribution': sentiment_counts.to_dict(),
        'crosstab': crosstab.to_dict()
    }

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ABSAä¸‰å…ƒç»„æå–')
    parser.add_argument('--dataset', type=str, default='chnsenticorp',
                        choices=['chnsenticorp', 'waimai10k', 'ecommerce'],
                        help='æ•°æ®é›†åç§°')
    parser.add_argument('--domain', type=str, default='hotel',
                        choices=['hotel', 'food', 'ecommerce'],
                        help='é¢†åŸŸç±»å‹')
    parser.add_argument('--sample-size', type=int, default=50,
                        help='åˆ†ææ ·æœ¬æ•°é‡')
    
    args = parser.parse_args()
    
    print(f"\n{'='*70}")
    print(f"ABSAä¸‰å…ƒç»„æå–".center(70))
    print(f"æ•°æ®é›†: {args.dataset} | é¢†åŸŸ: {args.domain}".center(70))
    print(f"{'='*70}")
    
    try:
        # åŠ è½½æ•°æ®
        base_dir = Path(__file__).parent.parent
        
        if args.dataset == 'chnsenticorp':
            data_path = base_dir / 'NLPæ•°æ®é›†/é…’åº—è¯„è®ºæ•°æ®/ChnSentiCorp_htl_all.csv'
        elif args.dataset == 'waimai10k':
            data_path = base_dir / 'NLPæ•°æ®é›†/å¤–å–è¯„è®ºæ•°æ®/waimai_10k.csv'
        elif args.dataset == 'ecommerce':
            data_path = base_dir / 'NLPæ•°æ®é›†/ç”µå•†è¯„è®ºæ•°æ®/online_shopping_10_cats.csv'
        else:
            raise ValueError(f"æœªçŸ¥æ•°æ®é›†: {args.dataset}")
        
        print(f"\nğŸ“Š åŠ è½½æ•°æ®...")
        df = pd.read_csv(data_path)
        
        # éšæœºé‡‡æ ·
        sample_df = df.sample(min(len(df), args.sample_size), random_state=42)
        texts = sample_df['review'].tolist()
        
        print(f"   æ ·æœ¬æ•°: {len(texts)}")
        
        # åˆå§‹åŒ–æå–å™¨
        print(f"\nğŸš€ åˆå§‹åŒ–ABSAæå–å™¨...")
        extractor = ABSAExtractor()
        print("   âœ… è¿æ¥æˆåŠŸ")
        
        # æ‰¹é‡æå–
        results_df = extractor.extract_batch(texts, args.domain, args.sample_size)
        
        # åˆ†æåˆ†å¸ƒ
        stats = analyze_absa_distribution(results_df)
        
        # ä¿å­˜ç»“æœ
        output_dir = Path(__file__).parent.parent / 'results' / 'absa' / args.dataset
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_path = output_dir / 'absa_triplets.csv'
        
        # å±•å¼€ä¸‰å…ƒç»„ä¸ºå•ç‹¬çš„è¡Œï¼ˆä¾¿äºåˆ†æï¼‰
        expanded_rows = []
        for idx, row in results_df.iterrows():
            if isinstance(row['triplets'], list) and len(row['triplets']) > 0:
                for t in row['triplets']:
                    expanded_rows.append({
                        'text': row['text'],
                        'aspect': t.get('aspect', ''),
                        'opinion': t.get('opinion', ''),
                        'sentiment': t.get('sentiment', '')
                    })
            else:
                expanded_rows.append({
                    'text': row['text'],
                    'aspect': '',
                    'opinion': '',
                    'sentiment': ''
                })
        
        expanded_df = pd.DataFrame(expanded_rows)
        expanded_df.to_csv(results_path, index=False)
        
        # ä¿å­˜åŸå§‹è¾“å‡º
        raw_path = output_dir / 'absa_raw_outputs.json'
        with open(raw_path, 'w', encoding='utf-8') as f:
            json.dump(results_df.to_dict('records'), f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        if stats:
            stats_path = output_dir / 'absa_statistics.json'
            with open(stats_path, 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆå¯è¯»æŠ¥å‘Š
        report_path = output_dir / 'absa_report.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# ABSAä¸‰å…ƒç»„æå–æŠ¥å‘Š\n\n")
            f.write(f"**æ•°æ®é›†**: {args.dataset}\n")
            f.write(f"**é¢†åŸŸ**: {args.domain}\n")
            f.write(f"**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**åˆ†ææ ·æœ¬æ•°**: {len(results_df)}\n")
            
            if stats:
                f.write(f"**æå–ä¸‰å…ƒç»„æ€»æ•°**: {stats['total_triplets']}\n")
                f.write(f"**å”¯ä¸€æ–¹é¢è¯æ•°**: {stats['unique_aspects']}\n\n")
            
            f.write("---\n\n## ç¤ºä¾‹ç»“æœ\n\n")
            
            # å±•ç¤ºå‰5ä¸ªç¤ºä¾‹
            for idx, row in results_df.head(5).iterrows():
                f.write(f"### ç¤ºä¾‹ {idx + 1}\n\n")
                f.write(f"**è¯„è®º**: {row['text']}\n\n")
                f.write(f"**æå–çš„ä¸‰å…ƒç»„** ({row['n_triplets']}ä¸ª):\n\n")
                
                if isinstance(row['triplets'], list) and len(row['triplets']) > 0:
                    for t in row['triplets']:
                        f.write(f"- æ–¹é¢: **{t.get('aspect', '')}** | è§‚ç‚¹: _{t.get('opinion', '')}_ | æƒ…æ„Ÿ: **{t.get('sentiment', '')}**\n")
                else:
                    f.write("_æœªæå–åˆ°ä¸‰å…ƒç»„_\n")
                
                f.write("\n---\n\n")
        
        print(f"\nâœ… æå–å®Œæˆï¼")
        print(f"   ç»“æœå·²ä¿å­˜è‡³: {output_dir}")
        print(f"   - ä¸‰å…ƒç»„CSV: {results_path}")
        print(f"   - ç»Ÿè®¡ä¿¡æ¯: {output_dir / 'absa_statistics.json'}")
        print(f"   - æŠ¥å‘Šæ–‡ä»¶: {report_path}")
        print(f"{'='*70}\n")
        
    except Exception as e:
        print(f"\nâŒ æå–å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        exit(1)

if __name__ == '__main__':
    main()
