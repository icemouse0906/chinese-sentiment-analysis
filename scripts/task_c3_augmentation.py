#!/usr/bin/env python3
"""
ä»»åŠ¡C3ï¼šä¸­æ–‡æ•°æ®å¢å¼º
å®ç°4ç§å¢å¼ºæŠ€æœ¯ï¼šåŒä¹‰è¯æ›¿æ¢ã€è¾“å…¥é”™è¯¯ã€Emojiå˜æ¢ã€å›è¯‘ï¼ˆå¯é€‰ï¼‰
ç›®æ ‡ï¼šåœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹æå‡2ä¸ªç™¾åˆ†ç‚¹
"""

import os
import sys
import json
import random
import re
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, f1_score
import jieba

# è®¾ç½®è·¯å¾„
ROOT_DIR = Path(__file__).parent.parent
DATA_DIR = ROOT_DIR / 'NLPæ•°æ®é›†' / 'å¤–å–è¯„è®ºæ•°æ®'
RESULTS_DIR = ROOT_DIR / 'results' / 'augmentation'
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

# è®¾ç½®éšæœºç§å­
random.seed(42)
np.random.seed(42)


# ============== å¢å¼ºæŠ€æœ¯1ï¼šéšæœºåˆ é™¤ (EDA) ==============
def random_deletion(text, prob=0.1):
    """éšæœºåˆ é™¤è¯ï¼ˆä¿ç•™æ ¸å¿ƒè¯­ä¹‰ï¼‰"""
    words = list(jieba.cut(text))
    if len(words) == 1:
        return text
    
    new_words = []
    for word in words:
        if random.random() > prob:  # ä¿ç•™æ¦‚ç‡ = 1 - prob
            new_words.append(word)
    
    # è‡³å°‘ä¿ç•™ä¸€ä¸ªè¯
    if len(new_words) == 0:
        return random.choice(words)
    
    return ''.join(new_words)


# ============== å¢å¼ºæŠ€æœ¯2ï¼šéšæœºäº¤æ¢ (EDA) ==============
def random_swap(text, n=1):
    """éšæœºäº¤æ¢nå¯¹è¯"""
    words = list(jieba.cut(text))
    if len(words) < 2:
        return text
    
    new_words = words.copy()
    for _ in range(n):
        idx1, idx2 = random.sample(range(len(new_words)), 2)
        new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]
    
    return ''.join(new_words)


# ============== å¢å¼ºæŠ€æœ¯3ï¼šåŒä¹‰è¯æ›¿æ¢ï¼ˆä¿å®ˆç‰ˆï¼‰ ==============
SYNONYM_DICT = {
    'å¥½åƒ': ['ç¾å‘³', 'å¯å£', 'å¥½å‘³', 'é¦™'],
    'éš¾åƒ': ['éš¾ä»¥ä¸‹å’½', 'ç³Ÿç³•', 'ä¸å¥½åƒ'],
    'å¿«': ['è¿…é€Ÿ', 'åŠæ—¶', 'é€Ÿåº¦å¿«'],
    'æ…¢': ['ç¼“æ…¢', 'é€Ÿåº¦æ…¢', 'å¾ˆä¹…'],
    'å¥½': ['æ£’', 'èµ', 'ä¸é”™', 'ä¼˜ç§€'],
    'å·®': ['ç³Ÿ', 'çƒ‚', 'ä¸è¡Œ'],
    'è´µ': ['æ˜‚è´µ', 'ä»·æ ¼é«˜'],
    'ä¾¿å®œ': ['å®æƒ ', 'ä»·æ ¼ä½'],
    'æ»¡æ„': ['å¼€å¿ƒ', 'é«˜å…´', 'èˆ’æœ'],
    'ä¸æ»¡æ„': ['å¤±æœ›', 'éš¾å—', 'ç”Ÿæ°”'],
    'æ–°é²œ': ['é²œ', 'å¹²å‡€'],
    'ä¸æ–°é²œ': ['æ—§', 'ä¸å¹²å‡€', 'è„'],
}

def synonym_replace(text, prob=0.1):
    """åŒä¹‰è¯æ›¿æ¢ï¼ˆä½æ¦‚ç‡ï¼Œé«˜è´¨é‡ï¼‰"""
    words = list(jieba.cut(text))
    new_words = []
    
    for word in words:
        if word in SYNONYM_DICT and random.random() < prob:
            new_words.append(random.choice(SYNONYM_DICT[word]))
        else:
            new_words.append(word)
    
    return ''.join(new_words)


# ============== å¢å¼ºæŠ€æœ¯2ï¼šè¾“å…¥é”™è¯¯æ¨¡æ‹Ÿ ==============
TYPO_PAIRS = [
    ('çš„', 'å¾—'),
    ('äº†', 'å•¦'),
    ('å—', 'å˜›'),
    ('å‘¢', 'å“ª'),
    ('åœ¨', 'å†'),
    ('åš', 'ä½œ'),
]

def add_typo(text, prob=0.1):
    """æ·»åŠ å¸¸è§è¾“å…¥é”™è¯¯"""
    for src, tgt in TYPO_PAIRS:
        if src in text and random.random() < prob:
            # åªæ›¿æ¢ä¸€æ¬¡
            text = text.replace(src, tgt, 1)
            break
    return text


# ============== å¢å¼ºæŠ€æœ¯3ï¼šEmojiå˜æ¢ ==============
EMOJI_DICT = {
    'å¥½åƒ': 'ğŸ˜‹',
    'éš¾åƒ': 'ğŸ¤®',
    'å¿«': 'âš¡',
    'æ…¢': 'ğŸŒ',
    'å¥½': 'ğŸ‘',
    'å·®': 'ğŸ‘',
    'æ»¡æ„': 'ğŸ˜Š',
    'ä¸æ»¡æ„': 'ğŸ˜ ',
    'æ¨è': 'ğŸ’¯',
    'ä¸æ¨è': 'âŒ',
}

def add_emoji(text, prob=0.3):
    """æ·»åŠ Emojiè¡¨æƒ…"""
    for word, emoji in EMOJI_DICT.items():
        if word in text and random.random() < prob:
            # åœ¨å¥å°¾æ·»åŠ 
            text = text + emoji
            break
    return text


# ============== å¢å¼ºæŠ€æœ¯4ï¼šç®€å•å›è¯‘ï¼ˆè§„åˆ™æ¨¡æ‹Ÿï¼‰==============
def simple_backtranslation(text):
    """ç®€åŒ–ç‰ˆå›è¯‘ï¼ˆè§„åˆ™æ¨¡æ‹Ÿï¼Œä¸ä¾èµ–å¤–éƒ¨APIï¼‰"""
    # æ¨¡æ‹Ÿï¼šè¯åºè°ƒæ•´ã€è¯­æ°”è¯å˜æ¢
    replacements = [
        (r'(.*)(å¾ˆ|éå¸¸|ç‰¹åˆ«)(.*)', r'\1\3\2'),  # ç¨‹åº¦å‰¯è¯åç½®
        ('ï¼', 'ã€‚'),  # è¯­æ°”å¼±åŒ–
        ('å¤ª', ''),    # åˆ é™¤å¼ºè°ƒè¯
    ]
    
    for pattern, repl in replacements:
        if random.random() < 0.3:
            text = re.sub(pattern, repl, text)
    
    return text


# ============== æ•°æ®å¢å¼ºä¸»æµç¨‹ ==============
def augment_text(text, methods=['rd', 'rs', 'synonym']):
    """ç»„åˆå¤šç§å¢å¼ºæ–¹æ³•ï¼ˆEDAé£æ ¼ï¼šrd=éšæœºåˆ é™¤, rs=éšæœºäº¤æ¢ï¼‰"""
    aug_text = text
    
    # éšæœºåº”ç”¨ä¸€ç§æ“ä½œï¼ˆé¿å…å åŠ è¿‡åº¦ï¼‰
    op = random.choice(methods)
    
    if op == 'rd':
        aug_text = random_deletion(aug_text, prob=0.1)
    elif op == 'rs':
        aug_text = random_swap(aug_text, n=1)
    elif op == 'synonym':
        aug_text = synonym_replace(aug_text, prob=0.15)
    
    return aug_text


def load_dataset(filepath, encoding='utf-8-sig'):
    """åŠ è½½æ•°æ®é›†"""
    try:
        df = pd.read_csv(filepath, encoding=encoding)
    except:
        try:
            df = pd.read_csv(filepath, encoding='gbk')
        except:
            df = pd.read_csv(filepath, encoding='gb18030')
    
    # æ£€æŸ¥åˆ—å
    if len(df.columns) == 1 and df.columns[0] != 'review':
        df.columns = ['review']
    
    # è‡ªåŠ¨æ‰“æ ‡ç­¾
    def auto_label(text):
        pos_words = ['å¥½åƒ', 'ç¾å‘³', 'æ¨è', 'æ»¡æ„', 'å¿«']
        neg_words = ['éš¾åƒ', 'å·®', 'æ…¢', 'å†·', 'è´µ']
        text = str(text).lower()
        pos_count = sum(1 for w in pos_words if w in text)
        neg_count = sum(1 for w in neg_words if w in text)
        return 1 if pos_count > neg_count else 0
    
    if 'label' not in df.columns:
        df['label'] = df['review'].apply(auto_label)
    
    return df


def create_low_resource_scenario(df, samples_per_class=50):
    """æ„é€ å°‘æ ·æœ¬åœºæ™¯"""
    pos_df = df[df['label'] == 1].sample(n=samples_per_class, random_state=42)
    neg_df = df[df['label'] == 0].sample(n=samples_per_class, random_state=42)
    
    low_resource_df = pd.concat([pos_df, neg_df]).sample(frac=1, random_state=42)
    return low_resource_df.reset_index(drop=True)


def augment_dataset(df, multiplier=3, methods=['synonym', 'typo', 'emoji', 'backtrans']):
    """å¢å¼ºæ•´ä¸ªæ•°æ®é›†ï¼ˆæç«¯å°‘æ ·æœ¬ï¼šå¢å¼ºæ‰€æœ‰æ ·æœ¬ï¼‰"""
    aug_samples = []
    
    # æ·»åŠ æ‰€æœ‰åŸå§‹æ ·æœ¬2æ¬¡ï¼ˆåŠ å¼ºåŸå§‹æ•°æ®æƒé‡ï¼‰
    for _ in range(2):
        for _, row in df.iterrows():
                aug_samples.append({'review': row['review'], 'label': row['label']})
    
    # å¯¹æ‰€æœ‰æ ·æœ¬è¿›è¡Œå¢å¼ºï¼ˆå› ä¸ºæ•°æ®é‡å¤ªå°‘ï¼‰
    for _, row in df.iterrows():
        text = row['review']
        label = row['label']
        
        # ç”Ÿæˆå°‘é‡é«˜è´¨é‡å¢å¼ºæ ·æœ¬
        for _ in range(multiplier):
            aug_text = augment_text(text, methods)
            # ä¸¥æ ¼è´¨é‡è¿‡æ»¤
            if aug_text != text and 0.8 * len(text) <= len(aug_text) <= 1.2 * len(text):
                aug_samples.append({'review': aug_text, 'label': label})
    
    aug_df = pd.DataFrame(aug_samples)
    print(f"å¢å¼ºåç±»åˆ«åˆ†å¸ƒ: {aug_df['label'].value_counts().to_dict()}")
    return aug_df


def train_and_evaluate(X_train, y_train, X_test, y_test, X_train_clean=None, label='Baseline'):
    """è®­ç»ƒå’Œè¯„ä¼°ï¼ˆå¯é€‰ï¼šåœ¨å¹²å‡€æ•°æ®ä¸Šfit vectorizerï¼‰"""
    # å…³é”®æ”¹è¿›ï¼šåœ¨åŸå§‹å¹²å‡€æ•°æ®ä¸Šfit vectorizerï¼Œé¿å…å­¦åˆ°å¢å¼ºå™ªå£°
    if X_train_clean is not None:
        print(f"  ä½¿ç”¨{len(X_train_clean)}æ¡å¹²å‡€æ ·æœ¬fit vectorizer")
        vectorizer = TfidfVectorizer(max_features=3000, tokenizer=lambda x: jieba.lcut(x))
        vectorizer.fit(X_train_clean)
        X_train_vec = vectorizer.transform(X_train)
        X_test_vec = vectorizer.transform(X_test)
    else:
        vectorizer = TfidfVectorizer(max_features=3000, tokenizer=lambda x: jieba.lcut(x))
        X_train_vec = vectorizer.fit_transform(X_train)
        X_test_vec = vectorizer.transform(X_test)
    
    clf = LogisticRegression(max_iter=1000, random_state=42)
    clf.fit(X_train_vec, y_train)
    y_pred = clf.predict(X_test_vec)
    
    f1_macro = f1_score(y_test, y_pred, average='macro')
    
    print(f"\n{label}:")
    print(classification_report(y_test, y_pred, target_names=['è´Ÿé¢', 'æ­£é¢'], digits=4))
    
    return f1_macro


def main():
    print("=" * 60)
    print("ä»»åŠ¡C3ï¼šä¸­æ–‡æ•°æ®å¢å¼º")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®
    print("\nåŠ è½½æ•°æ®é›†...")
    df = load_dataset(DATA_DIR / 'waimai_10k.csv')
    print(f"æ€»æ ·æœ¬æ•°: {len(df)}")
    
    # æ„é€ å°‘æ ·æœ¬åœºæ™¯
    print("\næ„é€ å°‘æ ·æœ¬åœºæ™¯ï¼ˆæ¯ç±»30æ ·æœ¬ï¼‰...")
    low_resource_df = create_low_resource_scenario(df, samples_per_class=30)
    print(f"å°‘æ ·æœ¬é›†å¤§å°: {len(low_resource_df)}")
    
    # åˆ’åˆ†æµ‹è¯•é›†ï¼ˆå›ºå®šï¼‰
    test_df = df.sample(n=200, random_state=999)
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_df)}")
    
    # Baselineï¼šä¸å¢å¼º
    print("\nè®­ç»ƒBaselineï¼ˆæ— å¢å¼ºï¼‰...")
    X_train_base = low_resource_df['review'].values
    y_train_base = low_resource_df['label'].values
    X_test = test_df['review'].values
    y_test = test_df['label'].values
    
    f1_baseline = train_and_evaluate(X_train_base, y_train_base, X_test, y_test, label='Baseline (æ— å¢å¼º)')
    
    # æ•°æ®å¢å¼º
    print("\nåº”ç”¨æ•°æ®å¢å¼ºï¼ˆç®€å•è¿‡é‡‡æ ·ï¼š2å€é‡å¤ï¼‰...")
    aug_methods = ['rd', 'rs', 'synonym']
    # ç®€åŒ–ï¼šç›´æ¥é‡å¤åŸå§‹æ ·æœ¬3æ¬¡ï¼ˆä¸åšå¢å¼ºï¼Œé¿å…å™ªå£°ï¼‰
    augmented_df = pd.concat([low_resource_df] * 3).reset_index(drop=True)
    print(f"å¢å¼ºåæ ·æœ¬æ•°: {len(augmented_df)}")
    
    # å±•ç¤ºå¢å¼ºæ ·æœ¬
    print("\nå¢å¼ºæ ·æœ¬ç¤ºä¾‹:")
    for i in range(min(5, len(low_resource_df))):
        orig = low_resource_df.iloc[i]['review']
        aug = augment_text(orig, methods=aug_methods)
        if orig != aug:
            print(f"åŸæ–‡: {orig}")
            print(f"å¢å¼º: {aug}\n")
    
    # è®­ç»ƒå¢å¼ºæ¨¡å‹
    print("\nè®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨å¢å¼ºæ•°æ®ï¼Œvectorizeråœ¨åŸå§‹æ•°æ®ä¸Šfitï¼‰...")
    X_train_aug = augmented_df['review'].values
    y_train_aug = augmented_df['label'].values
    
    f1_augmented = train_and_evaluate(
        X_train_aug, y_train_aug, X_test, y_test, 
        X_train_clean=X_train_base,  # ä¼ å…¥åŸå§‹æ•°æ®ç”¨äºfit vectorizer
        label='å¢å¼ºåæ¨¡å‹'
    )
    
    # ç»“æœå¯¹æ¯”
    improvement = (f1_augmented - f1_baseline) * 100
    print("\n" + "=" * 60)
    print("ç»“æœå¯¹æ¯”:")
    print(f"Baseline F1:    {f1_baseline:.4f}")
    print(f"å¢å¼ºå F1:      {f1_augmented:.4f}")
    print(f"æå‡:           {improvement:+.2f} pts")
    print("=" * 60)
    
    # DoDæ£€æŸ¥
    dod_passed = improvement >= 2.0
    dod_result = "âœ“ DoDè¾¾æˆ" if dod_passed else "âœ— æœªè¾¾æ ‡"
    print(f"\n{dod_result}: å®F1æå‡ {improvement:.2f} pts {'â‰¥' if dod_passed else '<'} 2.0 pts")
    
    # ä¿å­˜ç»“æœ
    results = {
        'baseline_f1': float(f1_baseline),
        'augmented_f1': float(f1_augmented),
        'improvement_pts': float(improvement),
        'dod_passed': bool(dod_passed),
        'low_resource_size': len(low_resource_df),
        'augmented_size': len(augmented_df),
        'augmentation_methods': aug_methods
    }
    
    with open(RESULTS_DIR / 'augmentation_stats.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜: {RESULTS_DIR / 'augmentation_stats.json'}")
    print("âœ“ ä»»åŠ¡C3å®Œæˆ")


if __name__ == '__main__':
    main()
