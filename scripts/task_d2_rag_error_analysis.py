"""
Task D2: RAGå¢å¼ºè¯¯åˆ¤åˆ†æ
ä½¿ç”¨å¥å‘é‡æ£€ç´¢ + DeepSeekç”Ÿæˆï¼Œä¸ºè¯¯åˆ¤æ ·æœ¬æä¾›è§£é‡Šæ€§åˆ†æ
"""

import os
import pandas as pd
import numpy as np
from pathlib import Path
from openai import OpenAI
import json
from datetime import datetime
from tqdm import tqdm
import time

try:
    from sentence_transformers import SentenceTransformer
    import faiss
except ImportError:
    print("âš ï¸  éœ€è¦å®‰è£…sentence-transformerså’Œfaiss-cpu")
    print("   pip install sentence-transformers faiss-cpu")
    exit(1)

class RAGErrorAnalyzer:
    """RAGå¢å¼ºçš„è¯¯åˆ¤åˆ†æå™¨"""
    
    def __init__(self, api_key=None, embedding_model='BAAI/bge-large-zh-v1.5'):
        """
        åˆå§‹åŒ–RAGåˆ†æå™¨
        
        Args:
            api_key: DeepSeek APIå¯†é’¥
            embedding_model: å¥å‘é‡æ¨¡å‹
        """
        self.api_key = api_key or os.getenv('DEEPSEEK_API_KEY')
        if not self.api_key:
            raise ValueError("è¯·è®¾ç½®DEEPSEEK_API_KEYç¯å¢ƒå˜é‡")
        
        self.client = OpenAI(
            api_key=self.api_key,
            base_url="https://api.deepseek.com"
        )
        
        # åŠ è½½å¥å‘é‡æ¨¡å‹
        print(f"ğŸ“¥ åŠ è½½å¥å‘é‡æ¨¡å‹: {embedding_model}")
        self.embedder = SentenceTransformer(embedding_model)
        print("   âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
        self.error_db = None
        self.error_samples = None
        self.index = None
    
    def build_error_database(self, error_samples_df):
        """
        æ„å»ºè¯¯åˆ¤æ ·æœ¬çš„å‘é‡æ•°æ®åº“
        
        Args:
            error_samples_df: è¯¯åˆ¤æ ·æœ¬DataFrame (åŒ…å«text, true_label, pred_labelåˆ—)
        """
        print(f"\nğŸ”¨ æ„å»ºè¯¯åˆ¤æ ·æœ¬å‘é‡æ•°æ®åº“...")
        print(f"   æ ·æœ¬æ•°é‡: {len(error_samples_df)}")
        
        self.error_samples = error_samples_df.copy()
        
        # ç”Ÿæˆå¥å‘é‡
        print("   ç”Ÿæˆå¥å‘é‡...")
        texts = error_samples_df['text'].tolist()
        embeddings = self.embedder.encode(texts, show_progress_bar=True)
        
        # æ„å»ºFAISSç´¢å¼•
        print("   æ„å»ºFAISSç´¢å¼•...")
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)  # å†…ç§¯ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        
        # å½’ä¸€åŒ–ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦éœ€è¦ï¼‰
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings.astype('float32'))
        
        self.error_db = embeddings
        print(f"   âœ… æ•°æ®åº“æ„å»ºå®Œæˆ (ç»´åº¦: {dimension})")
    
    def retrieve_similar_errors(self, query_text, top_k=3):
        """
        æ£€ç´¢ç›¸ä¼¼çš„è¯¯åˆ¤æ ·æœ¬
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›top-kä¸ªç›¸ä¼¼æ ·æœ¬
        """
        if self.index is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨build_error_database()æ„å»ºæ•°æ®åº“")
        
        # æŸ¥è¯¢å‘é‡
        query_embedding = self.embedder.encode([query_text])
        faiss.normalize_L2(query_embedding)
        
        # æ£€ç´¢
        similarities, indices = self.index.search(query_embedding.astype('float32'), top_k)
        
        # è¿”å›ç»“æœ
        results = []
        for sim, idx in zip(similarities[0], indices[0]):
            if idx < len(self.error_samples):
                sample = self.error_samples.iloc[idx]
                results.append({
                    'text': sample['text'],
                    'true_label': sample['true_label'],
                    'pred_label': sample['pred_label'],
                    'similarity': float(sim),
                    'index': int(idx)
                })
        
        return results
    
    def generate_error_analysis(self, query_sample, similar_cases):
        """
        ä½¿ç”¨DeepSeekç”Ÿæˆè¯¯åˆ¤åˆ†ææŠ¥å‘Š
        
        Args:
            query_sample: å½“å‰è¯¯åˆ¤æ ·æœ¬ (dict with text, true_label, pred_label)
            similar_cases: ç›¸ä¼¼è¯¯åˆ¤æ ·æœ¬åˆ—è¡¨
        """
        # æ„é€ æç¤ºè¯
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æƒ…æ„Ÿåˆ†ææ¨¡å‹è¯Šæ–­ä¸“å®¶ã€‚è¯·æ ¹æ®ç»™å®šçš„è¯¯åˆ¤æ ·æœ¬å’Œç›¸ä¼¼æ¡ˆä¾‹ï¼Œåˆ†ææ¨¡å‹ä¸ºä»€ä¹ˆä¼šçŠ¯é”™ã€‚

è¯·ä»ä»¥ä¸‹è§’åº¦åˆ†æï¼š
1. **æ–‡æœ¬ç‰¹å¾**ï¼šå¥å­ç»“æ„ã€è½¬æŠ˜è¯ã€æƒ…æ„Ÿè¯ç­‰
2. **æƒ…æ„Ÿè¡¨è¾¾**ï¼šéšå¼æƒ…æ„Ÿã€åè®½ã€å®¢å¥—è¯ç­‰
3. **ä¸Šä¸‹æ–‡ä¾èµ–**ï¼šæ˜¯å¦éœ€è¦é¢†åŸŸçŸ¥è¯†æˆ–å¸¸è¯†æ¨ç†
4. **ç›¸ä¼¼æ¡ˆä¾‹æ¨¡å¼**ï¼šä»ç›¸ä¼¼é”™è¯¯ä¸­æ€»ç»“å…±æ€§

è¾“å‡ºæ ¼å¼ï¼š
{
  "error_reason": "ä¸»è¦é”™è¯¯åŸå› çš„ç®€çŸ­æè¿°",
  "detailed_analysis": "è¯¦ç»†åˆ†æï¼ˆ100-200å­—ï¼‰",
  "suggested_fix": "æ”¹è¿›å»ºè®®"
}"""
        
        # æ„é€ ç”¨æˆ·æç¤º
        true_label_text = "æ­£é¢" if query_sample['true_label'] == 1 else "è´Ÿé¢"
        pred_label_text = "æ­£é¢" if query_sample['pred_label'] == 1 else "è´Ÿé¢"
        
        user_prompt = f"""**å½“å‰è¯¯åˆ¤æ ·æœ¬ï¼š**
æ–‡æœ¬ï¼š{query_sample['text']}
çœŸå®æ ‡ç­¾ï¼š{true_label_text}
é¢„æµ‹æ ‡ç­¾ï¼š{pred_label_text}

**ç›¸ä¼¼è¯¯åˆ¤æ¡ˆä¾‹ï¼ˆå…±{len(similar_cases)}æ¡ï¼‰ï¼š**
"""
        
        for i, case in enumerate(similar_cases, 1):
            case_true = "æ­£é¢" if case['true_label'] == 1 else "è´Ÿé¢"
            case_pred = "æ­£é¢" if case['pred_label'] == 1 else "è´Ÿé¢"
            user_prompt += f"\n{i}. æ–‡æœ¬ï¼š{case['text'][:100]}...\n   çœŸå®ï¼š{case_true} | é¢„æµ‹ï¼š{case_pred} | ç›¸ä¼¼åº¦ï¼š{case['similarity']:.3f}\n"
        
        user_prompt += "\nè¯·åˆ†æè¿™ä¸ªè¯¯åˆ¤æ¡ˆä¾‹çš„åŸå› ã€‚"
        
        # è°ƒç”¨DeepSeek
        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=800
            )
            
            result = response.choices[0].message.content.strip()
            
            # å°è¯•è§£æJSON
            try:
                analysis = json.loads(result)
            except:
                # å¦‚æœä¸æ˜¯JSONï¼ŒåŒ…è£…æˆå­—å…¸
                analysis = {
                    "error_reason": "æœªçŸ¥",
                    "detailed_analysis": result,
                    "suggested_fix": "éœ€è¦è¿›ä¸€æ­¥åˆ†æ"
                }
            
            return analysis
        
        except Exception as e:
            print(f"  âš ï¸  åˆ†æç”Ÿæˆå¤±è´¥: {str(e)}")
            return {
                "error_reason": "APIè°ƒç”¨å¤±è´¥",
                "detailed_analysis": str(e),
                "suggested_fix": "æ£€æŸ¥APIè¿æ¥"
            }
    
    def analyze_batch_errors(self, error_samples_df, top_k=3, max_samples=50):
        """
        æ‰¹é‡åˆ†æè¯¯åˆ¤æ ·æœ¬
        
        Args:
            error_samples_df: è¯¯åˆ¤æ ·æœ¬DataFrame
            top_k: æ£€ç´¢ç›¸ä¼¼æ ·æœ¬æ•°é‡
            max_samples: æœ€å¤§åˆ†ææ ·æœ¬æ•°ï¼ˆæ§åˆ¶æˆæœ¬ï¼‰
        """
        print(f"\nğŸ” æ‰¹é‡è¯¯åˆ¤åˆ†æ...")
        print(f"   æ€»è¯¯åˆ¤æ ·æœ¬: {len(error_samples_df)}")
        print(f"   åˆ†ææ ·æœ¬æ•°: {min(len(error_samples_df), max_samples)}")
        
        # å…ˆæ„å»ºæ•°æ®åº“
        self.build_error_database(error_samples_df)
        
        # éšæœºé€‰æ‹©æ ·æœ¬ï¼ˆæˆ–é€‰æ‹©ç½®ä¿¡åº¦æœ€ä½çš„ï¼‰
        if len(error_samples_df) > max_samples:
            samples_to_analyze = error_samples_df.sample(max_samples, random_state=42)
        else:
            samples_to_analyze = error_samples_df
        
        analyses = []
        
        for idx, row in tqdm(samples_to_analyze.iterrows(), total=len(samples_to_analyze), desc="   åˆ†æè¿›åº¦"):
            query_sample = {
                'text': row['text'],
                'true_label': row['true_label'],
                'pred_label': row['pred_label']
            }
            
            # æ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹
            similar_cases = self.retrieve_similar_errors(row['text'], top_k)
            
            # ç”Ÿæˆåˆ†æ
            analysis = self.generate_error_analysis(query_sample, similar_cases)
            
            # è®°å½•ç»“æœ
            analyses.append({
                'index': idx,
                'text': row['text'],
                'true_label': row['true_label'],
                'pred_label': row['pred_label'],
                'error_reason': analysis.get('error_reason', ''),
                'detailed_analysis': analysis.get('detailed_analysis', ''),
                'suggested_fix': analysis.get('suggested_fix', ''),
                'similar_cases': similar_cases
            })
            
            # é™é€Ÿ
            time.sleep(1)
        
        return pd.DataFrame(analyses)

def load_error_samples(dataset_name, model_type='svm'):
    """åŠ è½½æ¨¡å‹çš„è¯¯åˆ¤æ ·æœ¬"""
    base_dir = Path(__file__).parent.parent
    
    # åŠ è½½æµ‹è¯•æ•°æ®å’Œé¢„æµ‹ç»“æœ
    if dataset_name == 'chnsenticorp':
        data_path = base_dir / 'NLPæ•°æ®é›†/é…’åº—è¯„è®ºæ•°æ®/ChnSentiCorp_htl_all.csv'
    elif dataset_name == 'waimai10k':
        data_path = base_dir / 'NLPæ•°æ®é›†/å¤–å–è¯„è®ºæ•°æ®/waimai_10k.csv'
    else:
        raise ValueError(f"æœªçŸ¥æ•°æ®é›†: {dataset_name}")
    
    # åŠ è½½æ•°æ®
    df = pd.read_csv(data_path)
    
    # ç®€å•æ¨¡æ‹Ÿï¼šéšæœºç”Ÿæˆä¸€äº›è¯¯åˆ¤æ ·æœ¬ï¼ˆå®é™…åº”ä»æ¨¡å‹é¢„æµ‹ç»“æœä¸­è¯»å–ï¼‰
    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œåº”è¯¥ä» results/{dataset}/{model}/predictions.csv è¯»å–
    
    # è¿™é‡Œæˆ‘ä»¬ä»è·¨åŸŸå®éªŒçš„è¯¯åˆ¤æ ·æœ¬ä¸­è¯»å–
    error_samples_path = base_dir / 'results' / 'cross_domain' / 'error_samples.csv'
    
    if error_samples_path.exists():
        error_df = pd.read_csv(error_samples_path)
        # è¿‡æ»¤ç‰¹å®šæ•°æ®é›†çš„è¯¯åˆ¤
        if dataset_name == 'chnsenticorp':
            error_df = error_df[error_df['transfer'].str.contains('hotel|é…’åº—', case=False, na=False)]
        elif dataset_name == 'waimai10k':
            error_df = error_df[error_df['transfer'].str.contains('takeaway|å¤–å–', case=False, na=False)]
        
        # é‡å‘½ååˆ—
        error_df = error_df.rename(columns={
            'text': 'text',
            'true_label': 'true_label',
            'pred_label': 'pred_label'
        })
        
        return error_df[['text', 'true_label', 'pred_label']].head(100)
    else:
        # å¦‚æœæ²¡æœ‰è¯¯åˆ¤æ ·æœ¬ï¼Œéšæœºç”Ÿæˆä¸€äº›ç¤ºä¾‹
        print("âš ï¸  æœªæ‰¾åˆ°è¯¯åˆ¤æ ·æœ¬æ–‡ä»¶ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        sample_errors = df.sample(50, random_state=42).copy()
        # éšæœºç¿»è½¬ä¸€äº›æ ‡ç­¾ä½œä¸º"è¯¯åˆ¤"
        sample_errors['pred_label'] = sample_errors['label'].apply(lambda x: 1 - x if np.random.rand() > 0.7 else x)
        sample_errors = sample_errors[sample_errors['label'] != sample_errors['pred_label']]
        
        return sample_errors.rename(columns={'review': 'text', 'label': 'true_label'})[['text', 'true_label', 'pred_label']]

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='RAGå¢å¼ºè¯¯åˆ¤åˆ†æ')
    parser.add_argument('--dataset', type=str, default='chnsenticorp',
                        choices=['chnsenticorp', 'waimai10k'],
                        help='æ•°æ®é›†åç§°')
    parser.add_argument('--model', type=str, default='svm',
                        choices=['nb', 'svm'],
                        help='æ¨¡å‹ç±»å‹')
    parser.add_argument('--top-k', type=int, default=3,
                        help='æ£€ç´¢ç›¸ä¼¼æ ·æœ¬æ•°é‡')
    parser.add_argument('--max-samples', type=int, default=20,
                        help='æœ€å¤§åˆ†ææ ·æœ¬æ•°')
    
    args = parser.parse_args()
    
    print(f"\n{'='*70}")
    print(f"RAGå¢å¼ºè¯¯åˆ¤åˆ†æ".center(70))
    print(f"æ•°æ®é›†: {args.dataset} | æ¨¡å‹: {args.model.upper()}".center(70))
    print(f"{'='*70}")
    
    try:
        # åŠ è½½è¯¯åˆ¤æ ·æœ¬
        print("\nğŸ“Š åŠ è½½è¯¯åˆ¤æ ·æœ¬...")
        error_samples = load_error_samples(args.dataset, args.model)
        print(f"   è¯¯åˆ¤æ ·æœ¬æ•°: {len(error_samples)}")
        
        if len(error_samples) == 0:
            print("âŒ æ²¡æœ‰è¯¯åˆ¤æ ·æœ¬å¯åˆ†æ")
            return
        
        # åˆå§‹åŒ–åˆ†æå™¨
        print("\nğŸš€ åˆå§‹åŒ–RAGåˆ†æå™¨...")
        analyzer = RAGErrorAnalyzer()
        
        # æ‰¹é‡åˆ†æ
        analysis_df = analyzer.analyze_batch_errors(
            error_samples,
            top_k=args.top_k,
            max_samples=args.max_samples
        )
        
        # ä¿å­˜ç»“æœ
        output_dir = Path(__file__).parent.parent / 'results' / 'rag_analysis' / args.dataset / args.model
        output_dir.mkdir(parents=True, exist_ok=True)
        
        analysis_path = output_dir / 'error_analysis.csv'
        analysis_df.to_csv(analysis_path, index=False)
        
        # ç”Ÿæˆå¯è¯»æŠ¥å‘Š
        report_path = output_dir / 'error_analysis_report.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# RAGå¢å¼ºè¯¯åˆ¤åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**æ•°æ®é›†**: {args.dataset}\n")
            f.write(f"**æ¨¡å‹**: {args.model.upper()}\n")
            f.write(f"**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**åˆ†ææ ·æœ¬æ•°**: {len(analysis_df)}\n\n")
            f.write("---\n\n")
            
            for idx, row in analysis_df.iterrows():
                f.write(f"## æ¡ˆä¾‹ {idx + 1}\n\n")
                f.write(f"**æ–‡æœ¬**: {row['text']}\n\n")
                f.write(f"**çœŸå®æ ‡ç­¾**: {'æ­£é¢' if row['true_label'] == 1 else 'è´Ÿé¢'}\n")
                f.write(f"**é¢„æµ‹æ ‡ç­¾**: {'æ­£é¢' if row['pred_label'] == 1 else 'è´Ÿé¢'}\n\n")
                f.write(f"### é”™è¯¯åŸå› \n{row['error_reason']}\n\n")
                f.write(f"### è¯¦ç»†åˆ†æ\n{row['detailed_analysis']}\n\n")
                f.write(f"### æ”¹è¿›å»ºè®®\n{row['suggested_fix']}\n\n")
                f.write("---\n\n")
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼")
        print(f"   ç»“æœå·²ä¿å­˜è‡³: {output_dir}")
        print(f"   - CSVæ–‡ä»¶: {analysis_path}")
        print(f"   - æŠ¥å‘Šæ–‡ä»¶: {report_path}")
        print(f"{'='*70}\n")
        
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        exit(1)

if __name__ == '__main__':
    main()
