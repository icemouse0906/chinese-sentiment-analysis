# Pythonæ–°å¢è„šæœ¬ä»£ç æ±‡æ€»

---

## 1. FastAPIæ¨ç†æœåŠ¡ï¼ˆapi/predict.pyï¼‰
```python
from fastapi import FastAPI, Request
from pydantic import BaseModel
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import uvicorn

app = FastAPI()

class Item(BaseModel):
    text: str

# åŠ è½½æ¨¡å‹ï¼ˆç¤ºä¾‹ï¼šMacBERTï¼‰
MODEL_NAME = 'hfl/chinese-macbert-base'
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)
model.eval()

@app.post('/predict')
def predict(item: Item):
    inputs = tokenizer(item.text, return_tensors='pt', truncation=True, padding='max_length', max_length=128)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()[0]
        label = int(probs.argmax())
        conf = float(probs[label])
    return {'label': label, 'confidence': conf}

if __name__ == '__main__':
    uvicorn.run(app, host='0.0.0.0', port=8000)
```

---

## 2. ä¸»æµç¨‹ç»Ÿä¸€å…¥å£ï¼ˆrun.pyï¼‰
```python
import argparse
import sys
import os
from pathlib import Path

# æ”¯æŒçš„æ•°æ®é›†ä¸æ¨¡å¼
DATASETS = {
    'hotel': 'NLPæ•°æ®é›†/é…’åº—è¯„è®ºæ•°æ®/ChnSentiCorp_htl_all.csv',
    'ecommerce': 'NLPæ•°æ®é›†/ç”µå•†è¯„è®ºæ•°æ®/online_shopping_10_cats.csv',
    'waimai': 'NLPæ•°æ®é›†/å¤–å–è¯„è®ºæ•°æ®/waimai_10k.csv'
}

MODELS = ['nb', 'svm']

STAGES = ['eda', 'label', 'train', 'lda', 'report']


def main():
    parser = argparse.ArgumentParser(description='ç»Ÿä¸€å…¥å£ï¼šä¸­æ–‡æƒ…æ„Ÿåˆ†æä¸ä¸»é¢˜å»ºæ¨¡å®éªŒ')
    parser.add_argument('--dataset', choices=DATASETS.keys(), required=True, help='é€‰æ‹©æ•°æ®é›†')
    parser.add_argument('--model', choices=MODELS, default='nb', help='é€‰æ‹©åˆ†ç±»æ¨¡å‹')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--mode', choices=['true', 'pseudo'], default='pseudo', help='æ ‡ç­¾æ¨¡å¼ï¼štrue=çœŸæ ‡ç­¾ï¼Œpseudo=ä¼ªæ ‡ç­¾')
    parser.add_argument('--stage', choices=STAGES, default='report', help='æµç¨‹é˜¶æ®µ')
    args = parser.parse_args()

    # ç¯å¢ƒå˜é‡è®¾ç½®éšæœºç§å­
    os.environ['PYTHONHASHSEED'] = str(args.seed)

    # 1. EDAä¸é¢„å¤„ç†
    if args.stage == 'eda':
        os.system(f'python scripts/02_preprocess_and_eda.py')
        sys.exit(0)

    # 2. æ ‡ç­¾ç”Ÿæˆä¸åŸºçº¿åˆ†ç±»
    if args.stage == 'label':
        os.system(f'python scripts/03_label_and_model.py')
        sys.exit(0)

    # 3. è®­ç»ƒä¸è¯„æµ‹ï¼ˆä¼ªæ ‡ç­¾/çœŸæ ‡ç­¾åˆ†å¼€ï¼‰
    if args.stage == 'train':
        # ä¼ªæ ‡ç­¾æ¨¡å¼ï¼šç”¨03è„šæœ¬
        if args.mode == 'pseudo':
            os.system(f'python scripts/03_label_and_model.py')
        # çœŸæ ‡ç­¾æ¨¡å¼ï¼šç›´æ¥ç”¨åŸå§‹æ•°æ®é›†æ ‡ç­¾ï¼Œéœ€è¡¥å……å®ç°
        else:
            print('çœŸæ ‡ç­¾æ¨¡å¼æš‚æœªå®ç°è‡ªåŠ¨åŒ–ï¼Œè¯·æ‰‹åŠ¨è¿è¡Œç›¸å…³è„šæœ¬ã€‚')
        sys.exit(0)

    # 4. LDAä¸»é¢˜å»ºæ¨¡
    if args.stage == 'lda':
        os.system(f'python scripts/04_lda_by_sentiment.py')
        sys.exit(0)

    # 5. è‡ªåŠ¨æŠ¥å‘Š
    if args.stage == 'report':
        os.system(f'python scripts/10_balance_and_eda_report.py')
        sys.exit(0)

if __name__ == '__main__':
    main()
```

---

## 3. æ•°æ®å¢å¼ºè„šæœ¬ï¼ˆscripts/augment.pyï¼‰
```python
import argparse
import pandas as pd
import random
import re
from tqdm import tqdm

# ç®€å•åŒä¹‰è¯è¡¨ã€è¡¨æƒ…å½’ä¸€ã€é”™åˆ«å­—æ‰°åŠ¨
SYNONYMS = {'å¿«': ['è¿…é€Ÿ', 'é£å¿«'], 'å¥½': ['æ£’', 'ä¼˜ç§€'], 'æ…¢': ['è¿Ÿç¼“', 'æ‹–æ²“']}
EMOJI_MAP = {r'[ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜]': 'ç¬‘', r'[ğŸ˜¢ğŸ˜­]': 'å“­', r'[ğŸ‘]': 'èµ'}
TYPO_MAP = {'çš„': ['åœ°', 'å¾—'], 'äº†': ['å•¦', 'å–½']}


def synonym_replace(text, ratio=0.1):
    words = list(text)
    for i, w in enumerate(words):
        if w in SYNONYMS and random.random() < ratio:
            words[i] = random.choice(SYNONYMS[w])
    return ''.join(words)

def emoji_normalize(text):
    for pat, rep in EMOJI_MAP.items():
        text = re.sub(pat, rep, text)
    return text

def typo_perturb(text, ratio=0.05):
    words = list(text)
    for i, w in enumerate(words):
        if w in TYPO_MAP and random.random() < ratio:
            words[i] = random.choice(TYPO_MAP[w])
    return ''.join(words)

def main():
    parser = argparse.ArgumentParser(description='ä¸­æ–‡æ•°æ®å¢å¼º')
    parser.add_argument('--input', type=str, required=True, help='è¾“å…¥CSVï¼Œéœ€å«textåˆ—')
    parser.add_argument('--output', type=str, required=True, help='è¾“å‡ºCSV')
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--syn_ratio', type=float, default=0.1)
    parser.add_argument('--typo_ratio', type=float, default=0.05)
    args = parser.parse_args()
    random.seed(args.seed)
    df = pd.read_csv(args.input)
    texts = []
    for t in tqdm(df['text']):
        t1 = synonym_replace(t, args.syn_ratio)
        t2 = emoji_normalize(t1)
        t3 = typo_perturb(t2, args.typo_ratio)
        texts.append(t3)
    df['aug_text'] = texts
    df.to_csv(args.output, index=False)
    print(f'å¢å¼ºæ•°æ®å·²ä¿å­˜åˆ° {args.output}')

if __name__ == '__main__':
    main()
```

---

## 4. è·¨åŸŸè¯„æµ‹è„šæœ¬ï¼ˆscripts/cross_domain_eval.pyï¼‰
```python
import argparse
import pandas as pd
import numpy as np
from sklearn.metrics import f1_score

# è·¨åŸŸè¯„æµ‹ï¼šç”µå•†â†’å¤–å–â†’é…’åº—

def main():
    parser = argparse.ArgumentParser(description='è·¨åŸŸä¸å…¬å¹³æ€§è¯„æµ‹')
    parser.add_argument('--pred', type=str, required=True, help='é¢„æµ‹ç»“æœCSVï¼Œéœ€å«pred,labelåˆ—')
    parser.add_argument('--domain', type=str, required=True, help='åŸŸå')
    args = parser.parse_args()
    df = pd.read_csv(args.pred)
    macro_f1 = f1_score(df['label'], df['pred'], average='macro')
    print(f'{args.domain}åŸŸå®F1: {macro_f1:.3f}')
    # åç½®è¯å…¸æ‰«æ
    bias_words = ['åœ°åŒº','æ€§åˆ«','æ–¹è¨€','è¡¨æƒ…']
    bias_mask = df['text'].apply(lambda x: any(w in str(x) for w in bias_words))
    bias_acc = (df['pred']==df['label'])[bias_mask].mean() if bias_mask.sum()>0 else 0
    print(f'åç½®æ ·æœ¬è¯¯åˆ¤ç‡: {1-bias_acc:.3f} (æ ·æœ¬æ•°: {bias_mask.sum()})')

if __name__ == '__main__':
    main()
```

---

## 5. ABSAä¸‰å…ƒç»„è¯„æµ‹ï¼ˆscripts/eval_absa.pyï¼‰
```python
import argparse
import pandas as pd
import ast
from collections import Counter

def parse_triplets(s):
    # æ”¯æŒJSONæˆ–åˆ†å·åˆ†éš”
    if not s or pd.isna(s):
        return []
    try:
        if s.startswith('['):
            return ast.literal_eval(s)
        return [tuple(x.split(',')) for x in s.split(';') if x]
    except Exception:
        return []

def f1_score(pred, gold):
    pred_set = set(pred)
    gold_set = set(gold)
    tp = len(pred_set & gold_set)
    fp = len(pred_set - gold_set)
    fn = len(gold_set - pred_set)
    precision = tp / (tp + fp) if tp + fp > 0 else 0
    recall = tp / (tp + fn) if tp + fn > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0
    return precision, recall, f1

def main():
    parser = argparse.ArgumentParser(description='ABSAä¸‰å…ƒç»„è¯„æµ‹')
    parser.add_argument('--pred', type=str, required=True, help='é¢„æµ‹ç»“æœTSV')
    parser.add_argument('--gold', type=str, required=True, help='é‡‘æ ‡TSV')
    args = parser.parse_args()
    pred_df = pd.read_csv(args.pred, sep='\t')
    gold_df = pd.read_csv(args.gold, sep='\t')
    assert len(pred_df) == len(gold_df)
    p_total, r_total, f_total = 0, 0, 0
    n = len(pred_df)
    for i in range(n):
        pred_triplets = parse_triplets(pred_df.iloc[i]['triplets'])
        gold_triplets = parse_triplets(gold_df.iloc[i]['triplets'])
        p, r, f = f1_score(pred_triplets, gold_triplets)
        p_total += p
        r_total += r
        f_total += f
    print(f'ä¸‰å…ƒç»„F1: {f_total/n:.3f} Precision: {p_total/n:.3f} Recall: {r_total/n:.3f}')

if __name__ == '__main__':
    main()
```

---

## 6. W&Bæ—¥å¿—è„šæœ¬ï¼ˆscripts/log_wandb.pyï¼‰
```python
import wandb
import sys
import os
import json

# ç”¨æ³•ï¼špython scripts/log_wandb.py metrics.json
# metrics.json æ ¼å¼ï¼š{"f1":0.85, "accuracy":0.88, ...}

def main():
    if len(sys.argv) < 2:
        print('ç”¨æ³•: python scripts/log_wandb.py metrics.json')
        sys.exit(1)
    metrics_file = sys.argv[1]
    with open(metrics_file, 'r') as f:
        metrics = json.load(f)
    wandb.init(project="sentiment-analysis", name=os.path.basename(metrics_file))
    wandb.log(metrics)
    wandb.finish()

if __name__ == '__main__':
    main()
```

---

## 7. å¥å‘é‡ç”Ÿæˆä¸æ£€ç´¢ï¼ˆscripts/sentence_embedding.pyï¼‰
```python
import argparse
import pandas as pd
from sentence_transformers import SentenceTransformer
import numpy as np
import json

def main():
    parser = argparse.ArgumentParser(description='ä¸­æ–‡å¥å‘é‡ç”Ÿæˆä¸æ£€ç´¢')
    parser.add_argument('--input', type=str, required=True, help='è¾“å…¥CSVï¼Œéœ€å«textåˆ—')
    parser.add_argument('--output', type=str, default='output/sentence_embeddings.npy', help='è¾“å‡ºnpyæ–‡ä»¶')
    parser.add_argument('--model_name', type=str, default='BAAI/bge-base-zh', help='å¥å‘é‡æ¨¡å‹')
    args = parser.parse_args()
    df = pd.read_csv(args.input)
    model = SentenceTransformer(args.model_name)
    embeddings = model.encode(df['text'].tolist(), batch_size=32, show_progress_bar=True)
    np.save(args.output, embeddings)
    print(f'å¥å‘é‡å·²ä¿å­˜åˆ° {args.output}')
    # Top-5ç›¸ä¼¼å¥æ£€ç´¢ç¤ºä¾‹
    idx = 0
    query_emb = embeddings[idx]
    sims = np.dot(embeddings, query_emb) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(query_emb) + 1e-8)
    top5 = np.argsort(sims)[-6:-1][::-1]
    print('Top-5ç›¸ä¼¼å¥ï¼š')
    for i in top5:
        print(df.iloc[i]['text'])
    with open(args.output.replace('.npy','.top5.json'), 'w', encoding='utf-8') as f:
        json.dump([df.iloc[i]['text'] for i in top5], f, ensure_ascii=False, indent=2)

if __name__ == '__main__':
    main()
```

---

## 8. ä¸»é¢˜-æƒ…æ„Ÿè”åˆåˆ†æï¼ˆscripts/topic_sentiment_matrix.pyï¼‰
```python
import pandas as pd
import numpy as np
import argparse
import json
from collections import Counter

def main():
    parser = argparse.ArgumentParser(description='ä¸»é¢˜-æƒ…æ„Ÿè”åˆåˆ†æ')
    parser.add_argument('--topic_file', type=str, required=True, help='LDAä¸»é¢˜åˆ†é…ç»“æœCSVï¼Œéœ€å«topicåˆ—')
    parser.add_argument('--sentiment_file', type=str, required=True, help='æƒ…æ„Ÿæ ‡ç­¾CSVï¼Œéœ€å«sentiment_labelåˆ—')
    parser.add_argument('--output', type=str, default='output/topic_sentiment_matrix.csv', help='è¾“å‡ºçŸ©é˜µCSV')
    args = parser.parse_args()
    topic_df = pd.read_csv(args.topic_file)
    sent_df = pd.read_csv(args.sentiment_file)
    # å‡å®šä¸¤æ–‡ä»¶æœ‰ç›¸åŒç´¢å¼•æˆ–idåˆ—
    df = topic_df.join(sent_df, lsuffix='_topic', rsuffix='_sent')
    matrix = pd.crosstab(df['topic'], df['sentiment_label'])
    matrix.to_csv(args.output)
    print(f'ä¸»é¢˜-æƒ…æ„ŸçŸ©é˜µå·²ä¿å­˜åˆ° {args.output}')
    # å¯é€‰ï¼šè¾“å‡ºæ¯ä¸ªä¸»é¢˜çš„ä»£è¡¨æ ·æœ¬
    reps = {}
    for t in matrix.index:
        samples = df[df['topic']==t].sample(n=min(5, (df['topic']==t).sum()), random_state=42)
        reps[t] = samples['text'].tolist() if 'text' in samples.columns else samples.iloc[:,0].tolist()
    with open(args.output.replace('.csv','_samples.json'), 'w', encoding='utf-8') as f:
        json.dump(reps, f, ensure_ascii=False, indent=2)
    print(f'ä¸»é¢˜ä»£è¡¨æ ·æœ¬å·²ä¿å­˜åˆ° {args.output.replace('.csv','_samples.json')}')

if __name__ == '__main__':
    main()
```

---

## 9. å¼ºåŸºçº¿transformerè®­ç»ƒï¼ˆscripts/train_transformer.pyï¼‰
```python
import argparse
import os
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset, Dataset
import torch
import time

def main():
    parser = argparse.ArgumentParser(description='ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹åŸºçº¿è®­ç»ƒ')
    parser.add_argument('--model_name', type=str, default='hfl/chinese-roberta-wwm-ext', help='é¢„è®­ç»ƒæ¨¡å‹å')
    parser.add_argument('--train_file', type=str, required=True, help='è®­ç»ƒé›†CSVæ–‡ä»¶')
    parser.add_argument('--test_file', type=str, required=True, help='æµ‹è¯•é›†CSVæ–‡ä»¶')
    parser.add_argument('--text_col', type=str, default='tokens_join', help='æ–‡æœ¬åˆ—å')
    parser.add_argument('--label_col', type=str, default='sentiment_label', help='æ ‡ç­¾åˆ—å')
    parser.add_argument('--output_dir', type=str, default='./output/transformer', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--epochs', type=int, default=2)
    parser.add_argument('--batch_size', type=int, default=16)
    parser.add_argument('--seed', type=int, default=42)
    args = parser.parse_args()

    os.makedirs(args.output_dir, exist_ok=True)
    torch.manual_seed(args.seed)

    # åŠ è½½æ•°æ®
    import pandas as pd
    train_df = pd.read_csv(args.train_file)
    test_df = pd.read_csv(args.test_file)
    train_ds = Dataset.from_pandas(train_df)
    test_ds = Dataset.from_pandas(test_df)

    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    def preprocess(examples):
        return tokenizer(examples[args.text_col], truncation=True, padding='max_length', max_length=128)
    train_ds = train_ds.map(preprocess, batched=True)
    test_ds = test_ds.map(preprocess, batched=True)

    model = AutoModelForSequenceClassification.from_pretrained(args.model_name, num_labels=2)
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        evaluation_strategy='epoch',
        save_strategy='epoch',
        seed=args.seed,
        logging_dir=args.output_dir,
        report_to=['none'],
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=test_ds,
    )
    start = time.time()
    trainer.train()
    end = time.time()
    print(f"è®­ç»ƒæ—¶é•¿: {end-start:.1f}ç§’")
    metrics = trainer.evaluate()
    print(metrics)
    with open(os.path.join(args.output_dir, 'metrics.txt'), 'w') as f:
        f.write(str(metrics))
    torch.save(model.state_dict(), os.path.join(args.output_dir, 'model.pt'))

if __name__ == '__main__':
    main()
```

---

## 10. ä¸ç¡®å®šæ€§ä¸æ‹’è¯†è¯„ä¼°ï¼ˆscripts/uncertainty_reject.pyï¼‰
```python
import argparse
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.calibration import calibration_curve

def reliability_diagram(probs, labels, output):
    prob_true, prob_pred = calibration_curve(labels, probs, n_bins=10)
    plt.figure(figsize=(5,5))
    plt.plot(prob_pred, prob_true, marker='o', label='Reliability')
    plt.plot([0,1],[0,1],'--',label='Perfect')
    plt.xlabel('Predicted probability')
    plt.ylabel('True probability')
    plt.title('Reliability Diagram')
    plt.legend()
    plt.savefig(output)
    print(f'å¯é æ€§å›¾å·²ä¿å­˜åˆ° {output}')

def expected_calibration_error(probs, labels, n_bins=10):
    bins = np.linspace(0,1,n_bins+1)
    ece = 0
    for i in range(n_bins):
        mask = (probs >= bins[i]) & (probs < bins[i+1])
        if mask.sum() == 0: continue
        acc = labels[mask].mean()
        conf = probs[mask].mean()
        ece += abs(acc-conf) * mask.sum() / len(probs)
    return ece

def main():
    parser = argparse.ArgumentParser(description='ä¸ç¡®å®šæ€§ä¸æ‹’è¯†è¯„ä¼°')
    parser.add_argument('--probs', type=str, required=True, help='é¢„æµ‹æ¦‚ç‡CSVï¼Œéœ€å«prob,labelåˆ—')
    parser.add_argument('--output', type=str, default='output/reliability.png')
    parser.add_argument('--threshold', type=float, default=0.7, help='æ‹’è¯†é˜ˆå€¼')
    args = parser.parse_args()
    df = pd.read_csv(args.probs)
    probs = df['prob'].values
    labels = df['label'].values
    reliability_diagram(probs, labels, args.output)
    ece = expected_calibration_error(probs, labels)
    print(f'ECE: {ece:.4f}')
    # æ‹’è¯†ç­–ç•¥
    accept = (probs > args.threshold) | (probs < 1-args.threshold)
    acc = (df['pred']==df['label'])[accept].mean() if accept.sum()>0 else 0
    print(f'æ‹’è¯†åå‡†ç¡®ç‡: {acc:.3f}ï¼Œå¹³å‡å®¡æ ¸é‡: {1-accept.mean():.3f}')

if __name__ == '__main__':
    main()
```

---

## 11. å¼±ç›‘ç£æ ‡ç­¾ç”Ÿæˆï¼ˆscripts/weak_labeler.pyï¼‰
```python
import pandas as pd
import numpy as np
import argparse
import json

# è§„åˆ™ç¤ºä¾‹ï¼šè¯å…¸æ³•ã€æ¨¡å‹åˆ†æ•°æ³•ã€å¯å‘å¼æŠ•ç¥¨
POS_WORDS = set(['å¥½', 'æ£’', 'å¿«', 'æ–°é²œ', 'æ»¡æ„', 'èµ', 'å–œæ¬¢'])
NEG_WORDS = set(['æ…¢', 'å·®', 'å', 'è„', 'å¤±æœ›', 'ç ´æŸ', 'ä¸æ»¡æ„'])


def rule_label(text):
    # ç®€å•è¯å…¸æ³•
    for w in POS_WORDS:
        if w in text:
            return 1
    for w in NEG_WORDS:
        if w in text:
            return 0
    return -1  # æœªçŸ¥

def combine_labels(row, threshold=0.8):
    # è§„åˆ™ä¼˜å…ˆï¼Œæ¨¡å‹åˆ†æ•°è¾…åŠ©
    rule = rule_label(row['text'])
    model_score = row.get('model_score', None)
    if rule != -1:
        return rule, 'rule'
    if model_score is not None:
        if model_score >= threshold:
            return 1, 'model_high'
        elif model_score <= 1-threshold:
            return 0, 'model_low'
    return -1, 'abstain'


def main():
    parser = argparse.ArgumentParser(description='å¼±ç›‘ç£æ ‡ç­¾ç”Ÿæˆå™¨')
    parser.add_argument('--input', type=str, required=True, help='è¾“å…¥CSVï¼Œéœ€å«textåˆ—å’Œå¯é€‰model_scoreåˆ—')
    parser.add_argument('--output', type=str, required=True, help='è¾“å‡ºCSV')
    parser.add_argument('--threshold', type=float, default=0.8, help='æ¨¡å‹åˆ†æ•°ç½®ä¿¡åº¦é˜ˆå€¼')
    args = parser.parse_args()
    df = pd.read_csv(args.input)
    labels = []
    sources = []
    for _, row in df.iterrows():
        label, source = combine_labels(row, args.threshold)
        labels.append(label)
        sources.append(source)
    df['weak_label'] = labels
    df['label_source'] = sources
    df.to_csv(args.output, index=False)
    print(f'å·²ç”Ÿæˆå¼±ç›‘ç£æ ‡ç­¾ï¼Œä¿å­˜åˆ° {args.output}')

if __name__ == '__main__':
    main()
```

---

ï¼ˆå¦‚éœ€åˆ†æ¨¡å—å•ç‹¬å¤åˆ¶ï¼Œå¯ç›´æ¥æŒ‰åºå·é€‰å–ï¼‰
